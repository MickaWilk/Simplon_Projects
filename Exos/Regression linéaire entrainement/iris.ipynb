{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "df = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = df.values\n",
    "X = vals[:,0:4]\n",
    "y = vals[:,4]\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    " kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    " cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    " results.append(cv_results)\n",
    " names.append(name)\n",
    " print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "plt.boxplot(results, labels=names)\n",
    "plt.title('Algorithm Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(gamma='auto')\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_validation)\n",
    "accuracy_score(Y_validation, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        11\n",
      "Iris-versicolor       1.00      0.92      0.96        13\n",
      " Iris-virginica       0.86      1.00      0.92         6\n",
      "\n",
      "       accuracy                           0.97        30\n",
      "      macro avg       0.95      0.97      0.96        30\n",
      "   weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function classification_report in module sklearn.metrics._classification:\n",
      "\n",
      "classification_report(y_true, y_pred, *, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\n",
      "    Build a text report showing the main classification metrics.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <classification_report>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "        Estimated targets as returned by a classifier.\n",
      "    \n",
      "    labels : array-like of shape (n_labels,), default=None\n",
      "        Optional list of label indices to include in the report.\n",
      "    \n",
      "    target_names : list of str of shape (n_labels,), default=None\n",
      "        Optional display names matching the labels (same order).\n",
      "    \n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "    \n",
      "    digits : int, default=2\n",
      "        Number of digits for formatting output floating point values.\n",
      "        When ``output_dict`` is ``True``, this will be ignored and the\n",
      "        returned values will not be rounded.\n",
      "    \n",
      "    output_dict : bool, default=False\n",
      "        If True, return output as dict.\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "    \n",
      "    zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "        Sets the value to return when there is a zero division. If set to\n",
      "        \"warn\", this acts as 0, but warnings are also raised.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    report : str or dict\n",
      "        Text summary of the precision, recall, F1 score for each class.\n",
      "        Dictionary returned if output_dict is True. Dictionary has the\n",
      "        following structure::\n",
      "    \n",
      "            {'label 1': {'precision':0.5,\n",
      "                         'recall':1.0,\n",
      "                         'f1-score':0.67,\n",
      "                         'support':1},\n",
      "             'label 2': { ... },\n",
      "              ...\n",
      "            }\n",
      "    \n",
      "        The reported averages include macro average (averaging the unweighted\n",
      "        mean per label), weighted average (averaging the support-weighted mean\n",
      "        per label), and sample average (only for multilabel classification).\n",
      "        Micro average (averaging the total true positives, false negatives and\n",
      "        false positives) is only shown for multi-label or multi-class\n",
      "        with a subset of classes, because it corresponds to accuracy\n",
      "        otherwise and would be the same for all metrics.\n",
      "        See also :func:`precision_recall_fscore_support` for more details\n",
      "        on averages.\n",
      "    \n",
      "        Note that in binary classification, recall of the positive class\n",
      "        is also known as \"sensitivity\"; recall of the negative class is\n",
      "        \"specificity\".\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    precision_recall_fscore_support, confusion_matrix,\n",
      "    multilabel_confusion_matrix\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import classification_report\n",
      "    >>> y_true = [0, 1, 2, 2, 2]\n",
      "    >>> y_pred = [0, 0, 2, 2, 1]\n",
      "    >>> target_names = ['class 0', 'class 1', 'class 2']\n",
      "    >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "                  precision    recall  f1-score   support\n",
      "    <BLANKLINE>\n",
      "         class 0       0.50      1.00      0.67         1\n",
      "         class 1       0.00      0.00      0.00         1\n",
      "         class 2       1.00      0.67      0.80         3\n",
      "    <BLANKLINE>\n",
      "        accuracy                           0.60         5\n",
      "       macro avg       0.50      0.56      0.49         5\n",
      "    weighted avg       0.70      0.60      0.61         5\n",
      "    <BLANKLINE>\n",
      "    >>> y_pred = [1, 1, 0]\n",
      "    >>> y_true = [1, 1, 1]\n",
      "    >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
      "                  precision    recall  f1-score   support\n",
      "    <BLANKLINE>\n",
      "               1       1.00      0.67      0.80         3\n",
      "               2       0.00      0.00      0.00         0\n",
      "               3       0.00      0.00      0.00         0\n",
      "    <BLANKLINE>\n",
      "       micro avg       1.00      0.67      0.80         3\n",
      "       macro avg       0.33      0.22      0.27         3\n",
      "    weighted avg       1.00      0.67      0.80         3\n",
      "    <BLANKLINE>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(classification_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e588d92b224e11b16adbbadd39936dea13a6488171770263a646fc57f44563d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
